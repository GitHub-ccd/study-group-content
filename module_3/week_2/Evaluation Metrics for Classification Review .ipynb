{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Review \n",
    "\n",
    "![](https://raw.githubusercontent.com/learn-co-curriculum/dsc-logistic-regression-topic-questions/master/visuals/cnf_matrix.png?token=AK7GP7ETVQ2IEUER2E5I7ZC6YWIVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the confusion matrix up above, calculate precision, recall, and F-1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:22:55.838212Z",
     "start_time": "2020-05-13T20:22:55.835960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is  0.8823529411764706\n",
      "Recall is  0.7142857142857143\n",
      "F-1 is  0.7894736842105262\n"
     ]
    }
   ],
   "source": [
    "#Use python to answer question here \n",
    "\n",
    "TP = 30\n",
    "TN = 54\n",
    "FP = 4\n",
    "FN = 12\n",
    "\n",
    "Precision = TP / ( TP + FP)\n",
    "Recall = TP / ( TP + FN )\n",
    "F1 = (2 * Precision * Recall) / ( Precision + Recall)\n",
    "\n",
    "print('Precision is ', Precision)\n",
    "print('Recall is ', Recall)\n",
    "print('F-1 is ', F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is a real life example of when you would care more about recall than precision? Make sure to include information about errors in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:23:50.584656Z",
     "start_time": "2020-05-13T20:23:50.582274Z"
    }
   },
   "source": [
    "Your written answer here \n",
    "\n",
    "If someone is suffering from a disease and it would say that they are not, rather than someone not suffering from it \n",
    "and being said that they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pick the best ROC curve from this graph and explain your choice.\n",
    "Note: each ROC curve represents one model, each labeled with the feature(s) inside each model.\n",
    "![](https://raw.githubusercontent.com/learn-co-curriculum/dsc-logistic-regression-topic-questions/master/visuals/many_roc.png?token=AK7GP7AHF62PSLZVQDC5DWK6YWJCQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:25:51.688614Z",
     "start_time": "2020-05-13T20:25:51.686226Z"
    }
   },
   "source": [
    "# Your written answer here \n",
    "The pink line seems to be the best curve as it covers the maximum area and indicates the highest True Positive Rate. \n",
    "The line with 'All features' seem to predict the model better as compared with Age and Estimated Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:26:08.743094Z",
     "start_time": "2020-05-13T20:26:07.022922Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:29:50.564112Z",
     "start_time": "2020-05-13T20:29:50.534379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original classifier has an accuracy score of 0.956.\n",
      "The original classifier has an area under the ROC curve of 0.836.\n"
     ]
    }
   ],
   "source": [
    "network_df = pickle.load(open(\"sample_network_data.pkl\", \"rb\"))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop(\"Purchased\", axis=1)\n",
    "y = network_df[\"Purchased\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train,y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f\"The original classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.\")\n",
    "\n",
    "# get the area under the curve from an ROC curve\n",
    "y_score = model.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc = round(roc_auc_score(y_test, y_score), 3)\n",
    "print(f\"The original classifier has an area under the ROC curve of {auc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The model above has an accuracy score that might be too good to believe. Using y.value_counts(), explain how y is affecting the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:27:33.411864Z",
     "start_time": "2020-05-13T20:27:33.409188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    257\n",
       "1     13\n",
       "Name: Purchased, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your written answer here \n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see here that there is an imbalance between the value counts for 0's and 1's. The count for 0's are much more\n",
    "than 1's because of which it is able to predict the 0's better than the 1's. Hence the accuracy is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What methods would you use to address the issues mentioned up above in question 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:28:07.531026Z",
     "start_time": "2020-05-13T20:28:07.528443Z"
    }
   },
   "source": [
    "Your written answer here \n",
    "1. Use SMOTE.\n",
    "2. Use weights(Balanced)\n",
    "3. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
